{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://static.wixstatic.com/media/66c28f_db7a1ba3e35b4b17a6688472c889b7bf~mv2_d_2777_1254_s_2.png/v1/fill/w_710,h_320,al_c,q_85,usm_0.66_1.00_0.01/logo_yellow_white.webp\" width=1000, height=450>\n<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>","metadata":{"id":"tWJTCBu4Tveb"}},{"cell_type":"markdown","source":"# Text Summarization\n\nВсем привет! Сегодня мы познакомимся с задачей суммаризации текста на примере генерации \"сжатых\" новостей. Рассмотрим некоторые базовые решения и познакомимся с архитектурами нейросетей для решения задачи.\nДатасет: gazeta.ru\n\n\n`Ноутбук создан на основе семинара Гусева Ильи на кафедре компьютерной лингвистики ABBYY МФТИ.`\n\nЗагрузим датасет и необходимые библиотеки","metadata":{"id":"Kmb8UhIzOnfK"}},{"cell_type":"code","source":"!wget -q https://www.dropbox.com/s/43l702z5a5i2w8j/gazeta_train.txt\n!wget -q https://www.dropbox.com/s/k2egt3sug0hb185/gazeta_val.txt\n!wget -q https://www.dropbox.com/s/3gki5n5djs9w0v6/gazeta_test.txt","metadata":{"id":"OqkLTkFRfXvA","execution":{"iopub.status.busy":"2022-07-23T19:35:46.931315Z","iopub.execute_input":"2022-07-23T19:35:46.931697Z","iopub.status.idle":"2022-07-23T19:36:06.157002Z","shell.execute_reply.started":"2022-07-23T19:35:46.931656Z","shell.execute_reply":"2022-07-23T19:36:06.155823Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip -q install razdel networkx pymorphy2 nltk rouge==0.3.1 summa ","metadata":{"id":"SXS1sdYZCluU","outputId":"9fc943c7-ac7a-41b9-c8e1-7d5b5226acf7","execution":{"iopub.status.busy":"2022-07-23T19:36:06.161626Z","iopub.execute_input":"2022-07-23T19:36:06.161922Z","iopub.status.idle":"2022-07-23T19:36:15.914507Z","shell.execute_reply.started":"2022-07-23T19:36:06.161894Z","shell.execute_reply":"2022-07-23T19:36:15.913331Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"id":"Wa0NfryxbPUP"}},{"cell_type":"markdown","source":"Посмотрим на то, как устроен датасет","metadata":{"id":"eesnclfDDV3F"}},{"cell_type":"code","source":"!head -n 1 gazeta_train.txt\n!cat gazeta_train.txt | wc -l\n!cat gazeta_val.txt | wc -l\n!cat gazeta_test.txt | wc -l","metadata":{"id":"Mz6CZYKQhnd-","outputId":"537934f4-b1a2-4302-bf78-0031c0c25187","execution":{"iopub.status.busy":"2022-07-23T19:36:15.916585Z","iopub.execute_input":"2022-07-23T19:36:15.917011Z","iopub.status.idle":"2022-07-23T19:36:18.901356Z","shell.execute_reply.started":"2022-07-23T19:36:15.916968Z","shell.execute_reply":"2022-07-23T19:36:18.900242Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import json\nimport random\n\ndef read_gazeta_records(file_name, shuffle=True, sort_by_date=False):\n    assert shuffle != sort_by_date\n    records = []\n    with open(file_name, \"r\") as r:\n        for line in r:\n            records.append(json.loads(line))\n    if sort_by_date:\n        records.sort(key=lambda x: x[\"date\"])\n    if shuffle:\n        random.shuffle\n    return records","metadata":{"id":"5pZ2UGS2DGjH","execution":{"iopub.status.busy":"2022-07-23T19:36:18.904964Z","iopub.execute_input":"2022-07-23T19:36:18.905258Z","iopub.status.idle":"2022-07-23T19:36:18.913930Z","shell.execute_reply.started":"2022-07-23T19:36:18.905231Z","shell.execute_reply":"2022-07-23T19:36:18.912867Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_records = read_gazeta_records(\"gazeta_train.txt\")\nval_records = read_gazeta_records(\"gazeta_val.txt\")\ntest_records = read_gazeta_records(\"gazeta_test.txt\")","metadata":{"id":"GNDp-BunEA91","execution":{"iopub.status.busy":"2022-07-23T19:36:18.915931Z","iopub.execute_input":"2022-07-23T19:36:18.916803Z","iopub.status.idle":"2022-07-23T19:36:22.320959Z","shell.execute_reply.started":"2022-07-23T19:36:18.916767Z","shell.execute_reply":"2022-07-23T19:36:22.320003Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\nfrom rouge import Rouge\n\ndef calc_scores(references, predictions, metric=\"all\"):\n    print(\"Count:\", len(predictions))\n    print(\"Ref:\", references[-1])\n    print(\"Pred:\", predictions[-1])\n\n    if metric in (\"bleu\", \"all\"):\n        print(\"BLEU: \", corpus_bleu([[r] for r in references], predictions))\n    if metric in (\"rouge\", \"all\"):\n        rouge = Rouge()\n        scores = rouge.get_scores(predictions, references, avg=True)\n        print(\"ROUGE: \", scores)","metadata":{"id":"397gjsNfFBZ_","execution":{"iopub.status.busy":"2022-07-23T19:36:22.322656Z","iopub.execute_input":"2022-07-23T19:36:22.323018Z","iopub.status.idle":"2022-07-23T19:36:22.330267Z","shell.execute_reply.started":"2022-07-23T19:36:22.322983Z","shell.execute_reply":"2022-07-23T19:36:22.329338Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Extractive RNN","metadata":{"id":"aaJKNsUGFBaA"}},{"cell_type":"markdown","source":"### BPE\nДля начала сделаем BPE токенизацию","metadata":{"id":"l3izlm8HFBaC"}},{"cell_type":"code","source":"!pip install youtokentome","metadata":{"id":"2DMVtloWFBaC","outputId":"fea61dc6-4a9f-4877-e09e-d82119ec289b","execution":{"iopub.status.busy":"2022-07-23T19:36:22.331817Z","iopub.execute_input":"2022-07-23T19:36:22.332451Z","iopub.status.idle":"2022-07-23T19:36:31.861269Z","shell.execute_reply.started":"2022-07-23T19:36:22.332415Z","shell.execute_reply":"2022-07-23T19:36:31.860127Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import youtokentome as yttm\n\ndef train_bpe(records, model_path, model_type=\"bpe\", vocab_size=10000, lower=True):\n    temp_file_name = \"temp.txt\"\n    with open(temp_file_name, \"w\") as temp:\n        for record in records:\n            text, summary = record['text'], record['summary']\n            if lower:\n                summary = summary.lower()\n                text = text.lower()\n            if not text or not summary:\n                continue\n            temp.write(text + \"\\n\")\n            temp.write(summary + \"\\n\")\n    yttm.BPE.train(data=temp_file_name, vocab_size=vocab_size, model=model_path)\n\ntrain_bpe(train_records, \"BPE_model.bin\")","metadata":{"id":"Yg9T6q0wFBaF","execution":{"iopub.status.busy":"2022-07-23T19:36:31.864149Z","iopub.execute_input":"2022-07-23T19:36:31.864787Z","iopub.status.idle":"2022-07-23T19:36:50.229744Z","shell.execute_reply.started":"2022-07-23T19:36:31.864753Z","shell.execute_reply":"2022-07-23T19:36:50.228776Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Словарь\nСоставим словарь для индексации токенов","metadata":{"id":"jJFAJHTtFBaF"}},{"cell_type":"code","source":"bpe_processor = yttm.BPE('BPE_model.bin')\nvocabulary = bpe_processor.vocab()","metadata":{"id":"MueXtatmFBaG","execution":{"iopub.status.busy":"2022-07-23T19:36:50.234545Z","iopub.execute_input":"2022-07-23T19:36:50.239159Z","iopub.status.idle":"2022-07-23T19:36:50.290825Z","shell.execute_reply.started":"2022-07-23T19:36:50.239121Z","shell.execute_reply":"2022-07-23T19:36:50.289853Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Кэш oracle summary\nЗакэшируем oracle summary, чтобы не пересчитывать их каждый раз","metadata":{"id":"Z_C_p7tHFBaH"}},{"cell_type":"code","source":"from rouge import Rouge\nimport razdel\nfrom tqdm import tqdm\n\nimport copy\n\ndef build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n    '''\n    Жадное построение oracle summary\n    '''\n    gold_summary = gold_summary.lower() if lower else gold_summary\n    # Делим текст на предложения\n    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n    n_sentences = len(sentences)\n    oracle_summary_sentences = set()\n    \n    score = -1.0\n    summaries = []\n    for _ in range(n_sentences):\n        for i in range(n_sentences):\n            if i in oracle_summary_sentences:\n                continue\n            current_summary_sentences = copy.copy(oracle_summary_sentences)\n            # Добавляем какое-то предложения к уже существующему summary\n            current_summary_sentences.add(i)\n            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n            # Считаем метрики\n            current_score = calc_score(current_summary, gold_summary)\n            summaries.append((current_score, current_summary_sentences))\n        # Если получилось улучшить метрики с добавлением какого-либо предложения, то пробуем добавить ещё\n        # Иначе на этом заканчиваем\n        best_summary_score, best_summary_sentences = max(summaries)\n        if best_summary_score <= score:\n            break\n        oracle_summary_sentences = best_summary_sentences\n        score = best_summary_score\n    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n    return oracle_summary, oracle_summary_sentences\n\ndef calc_single_score(pred_summary, gold_summary, rouge):\n    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']\n\ndef add_oracle_summary_to_records(records, max_sentences=30, lower=True, nrows=1000):\n    rouge = Rouge()\n    for i, record in tqdm(enumerate(records), total=len(records)):\n        if i >= nrows:\n            break\n        text = record[\"text\"]\n        summary = record[\"summary\"]\n\n        summary = summary.lower() if lower else summary\n        sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n        oracle_summary, sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge),\n                                                                         lower=lower, max_sentences=max_sentences)\n        record[\"sentences\"] = sentences\n        record[\"oracle_sentences\"] = list(sentences_indicies)\n        record[\"oracle_summary\"] = oracle_summary\n\n    return records[:nrows]\n\next_train_records = add_oracle_summary_to_records(train_records, nrows=2048)\next_val_records = add_oracle_summary_to_records(val_records, nrows=256)\next_test_records = add_oracle_summary_to_records(test_records, nrows=256)","metadata":{"id":"Fp23tuPbFBaH","outputId":"b1c7528b-f222-4db0-a3ff-f22fc4e4293f","execution":{"iopub.status.busy":"2022-07-23T19:36:50.295450Z","iopub.execute_input":"2022-07-23T19:36:50.298112Z","iopub.status.idle":"2022-07-23T19:45:07.449152Z","shell.execute_reply.started":"2022-07-23T19:36:50.298073Z","shell.execute_reply":"2022-07-23T19:45:07.448171Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Составление батчей","metadata":{"id":"UlXXc8qUHC5m"}},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"id":"YATQKCuqHPo3","outputId":"fc3ffbb9-f204-41a6-8aab-6cf2353c41e1","execution":{"iopub.status.busy":"2022-07-23T19:45:07.450444Z","iopub.execute_input":"2022-07-23T19:45:07.451356Z","iopub.status.idle":"2022-07-23T19:45:08.001990Z","shell.execute_reply.started":"2022-07-23T19:45:07.451316Z","shell.execute_reply":"2022-07-23T19:45:08.000950Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import random\nimport math\nimport razdel\nimport torch\nimport numpy as np\nfrom rouge import Rouge\n\n\nclass BatchIterator():\n    def __init__(self, records, vocabulary, batch_size, bpe_processor, shuffle=True, lower=True, max_sentences=30, max_sentence_length=50, device=torch.device('cpu')):\n        self.records = records\n        self.num_samples = len(records)\n        self.batch_size = batch_size\n        self.bpe_processor = bpe_processor\n        self.shuffle = shuffle\n        self.batches_count = int(math.ceil(self.num_samples / batch_size))\n        self.lower = lower\n        self.rouge = Rouge()\n        self.vocabulary = vocabulary\n        self.max_sentences = max_sentences\n        self.max_sentence_length = max_sentence_length\n        self.device = device\n\n        self.pad_index = 2\n        \n    def __len__(self):\n        return self.batches_count\n    \n    def __iter__(self):\n        indices = np.arange(self.num_samples)\n        if self.shuffle:\n            np.random.shuffle(indices)\n\n        for start in range(0, self.num_samples, self.batch_size):\n            end = min(start + self.batch_size, self.num_samples)\n            batch_indices = indices[start:end]\n\n            batch_inputs = []\n            batch_outputs = []\n            max_sentence_length = 0\n            max_sentences = 0\n            batch_records = []\n\n            for data_ind in batch_indices:\n                \n                record = self.records[data_ind]\n                batch_records.append(record)\n                text = record[\"text\"]\n                summary = record[\"summary\"]\n                summary = summary.lower() if self.lower else summary\n\n                if \"sentences\" not in record:\n                    sentences = [sentence.text.lower() if self.lower else sentence.text for sentence in razdel.sentenize(text)][:self.max_sentences]\n                else:\n                    sentences = record[\"sentences\"]\n                max_sentences = max(len(sentences), max_sentences)\n                \n                # номера предложений, которые в нашем саммари\n                if \"oracle_sentences\" not in record:\n                    calc_score = lambda x, y: calc_single_score(x, y, self.rouge)\n                    sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=calc_score, lower=self.lower, max_sentences=self.max_sentences)[1]\n                else:   \n                    sentences_indicies = record[\"oracle_sentences\"]\n                \n                # inputs - индексы слов в предложении\n                inputs = [bpe_processor.encode(sentence)[:self.max_sentence_length] for sentence in sentences]\n                max_sentence_length = max(max_sentence_length, max([len(tokens) for tokens in inputs]))\n                \n                # получение метки класса предложения\n                outputs = [int(i in sentences_indicies) for i in range(len(sentences))]\n                batch_inputs.append(inputs)\n                batch_outputs.append(outputs)\n\n            tensor_inputs = torch.zeros((self.batch_size, max_sentences, max_sentence_length), dtype=torch.long, device=self.device)\n            tensor_outputs = torch.zeros((self.batch_size, max_sentences), dtype=torch.float32, device=self.device)\n            # we add index 2 for padding\n          \n            tensor_inputs = tensor_inputs.fill_(self.pad_index)\n            tensor_outputs = tensor_outputs.fill_(self.pad_index)\n\n            for i, inputs in enumerate(batch_inputs):\n                for j, sentence_tokens in enumerate(inputs):\n                    tensor_inputs[i][j][:len(sentence_tokens)] = torch.tensor(sentence_tokens, dtype=torch.int64)\n\n            for i, outputs in enumerate(batch_outputs):\n                tensor_outputs[i][:len(outputs)] = torch.LongTensor(outputs)\n\n            tensor_outputs = tensor_outputs.long()\n            yield {\n                'inputs': tensor_inputs,\n                'outputs': tensor_outputs,\n                'records': batch_records\n            }","metadata":{"id":"MNyxstTChK3C","execution":{"iopub.status.busy":"2022-07-23T19:45:08.003606Z","iopub.execute_input":"2022-07-23T19:45:08.004376Z","iopub.status.idle":"2022-07-23T19:45:08.025076Z","shell.execute_reply.started":"2022-07-23T19:45:08.004334Z","shell.execute_reply":"2022-07-23T19:45:08.024172Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_iterator = BatchIterator(ext_train_records, vocabulary, 32, bpe_processor, device=device)\nval_iterator = BatchIterator(ext_val_records, vocabulary, 32, bpe_processor, device=device, shuffle=False)\ntest_iterator = BatchIterator(ext_test_records, vocabulary, 32, bpe_processor, device=device, shuffle=False)","metadata":{"id":"5ug9MIObdi03","execution":{"iopub.status.busy":"2022-07-23T19:45:08.026426Z","iopub.execute_input":"2022-07-23T19:45:08.027025Z","iopub.status.idle":"2022-07-23T19:45:08.039977Z","shell.execute_reply.started":"2022-07-23T19:45:08.026988Z","shell.execute_reply":"2022-07-23T19:45:08.038925Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Extractor -  SummaRuNNer\n https://arxiv.org/pdf/1611.04230.pdf\n","metadata":{"id":"yPlJMg0_dQM-"}},{"cell_type":"markdown","source":"### Homework\n\n* В данной реализации в `outputs` в качестве padding используется индекс 0. Измените в функции \\_\\_iter__ индекс padding, чтобы он не совпадал с классом 0 или 1, например, 2.\n* В качестве criterion используйте `CrossEntropyLoss`вместо `BCEWithLogitsLoss`\n* Из-за смены criterion, вы уже должны подавать на вход criterion ни одно число, а logits для каждого класса. Перед подачей logits вы можете отфильтровать предсказания для класса padding. В этом пункте вам придется изменять файл `train_model.py`, а именно функциии `train` и `evaluate`.\n* Используйте два варианта обучения: c весами в `CrossEntropyLoss` и без\n* Также сравните `inference`, когда вы ранжируете logits, и когды вы выбирате предложения, у котрых logits > 0, в двух вариантах обучения. \n* Реализуйте дополнительно характеристику предложения `novelty`. Как влияет добавление `novelty` на качество summary?\n* Постарайтесь улучшить качество модели, полученной на семинаре: $BLEU \\approx 0.45$","metadata":{"id":"4BSsnfe4t1uK"}},{"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom torch.nn.utils.rnn import pack_padded_sequence as pack\nfrom torch.nn.utils.rnn import pad_packed_sequence as unpack\n\nclass SentenceEncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_dim, hidden_size, n_layers=3, dropout=0.3, bidirectional=True):\n        super().__init__()\n\n        num_directions = 2 if bidirectional else 1\n        assert hidden_size % num_directions == 0\n        hidden_size = hidden_size // num_directions\n\n        self.embedding_dim = embedding_dim\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n\n        self.embedding_layer = nn.Embedding(input_size, embedding_dim)\n        self.rnn_layer = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n        self.dropout_layer = nn.Dropout(dropout)\n\n    def forward(self, inputs, hidden=None):\n        embedded = self.dropout_layer(self.embedding_layer(inputs))\n        outputs, _ = self.rnn_layer(embedded, hidden)\n        sentences_embeddings = torch.mean(outputs, 1)\n        # [batch_size, hidden_size]\n        return sentences_embeddings\n\nclass SentenceTaggerRNN(nn.Module):\n    def __init__(self,\n                 vocabulary_size,\n                 use_novelty=True,\n                 token_embedding_dim=128,\n                 sentence_encoder_hidden_size=256,\n                 hidden_size=256,\n                 bidirectional=True,\n                 sentence_encoder_n_layers=2,\n                 sentence_encoder_dropout=0.3,\n                 sentence_encoder_bidirectional=True,\n                 n_layers=2,\n                 dropout=0.3):\n        \n        super().__init__()\n\n        num_directions = 2 if bidirectional else 1\n        assert hidden_size % num_directions == 0\n        hidden_size = hidden_size // num_directions\n\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n\n        self.sentence_encoder = SentenceEncoderRNN(vocabulary_size, token_embedding_dim,\n                                                   sentence_encoder_hidden_size, sentence_encoder_n_layers, \n                                                   sentence_encoder_dropout, sentence_encoder_bidirectional)\n        \n        self.rnn_layer = nn.LSTM(sentence_encoder_hidden_size, hidden_size, n_layers, dropout=dropout,\n                           bidirectional=bidirectional, batch_first=True)\n        \n        self.dropout_layer = nn.Dropout(dropout)\n        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n        self.novelty_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n        self.tanh_layer = nn.Tanh()\n\n        self.use_novelty = use_novelty\n\n    def forward(self, inputs, hidden=None):\n        # parameters of the probability\n        content = 0\n        salience = 0\n        novelty = 0\n\n        # [batch_size, seq num, seq_len]\n        batch_size = inputs.size(0)\n        sentences_count = inputs.size(1)\n        tokens_count = inputs.size(2)\n        inputs = inputs.reshape(-1, tokens_count)\n        # [batch_size * seq num, seq_len]\n\n        embedded_sentences = self.sentence_encoder(inputs)\n        embedded_sentences = self.dropout_layer(embedded_sentences.reshape(batch_size, sentences_count, -1))\n        # [batch_size *  seq num, seq_len, hidden_size] -> [batch_size, seq num, hidden_size]\n\n        outputs, _ = self.rnn_layer(embedded_sentences, hidden)\n        # [batch_size, seq num, hidden_size]\n\n        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n        # [batch_size, hidden_size]\n\n        # W * h^T\n        content = self.content_linear_layer(outputs).squeeze(2) # 1-representation\n        # [batch_size, seq num]\n\n        # h^T * W * d\n        salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2) # 2-representation\n        # [batch_size, seq num, hidden_size] * [batch_size, hidden_size, 1] = [batch_size, seq num, ]\n\n        # at every step add novelty to prediction of the sentence\n        predictions = content + salience\n        novelty = torch.zeros((batch_size, sentences_count)).to(device)\n\n        if self.use_novelty:\n            # 0) initialize summary_representation and novelty by zeros\n            probabilities = torch.zeros((batch_size, sentences_count)).to(device)\n            summary_representation = torch.zeros(outputs.shape).to(device)\n            for sentence_num in range(sentences_count):\n                # 1) take sentence_num_state from outputs(representation of the sentence with number sentence_num)\n                # 2) calculate novelty for current sentence\n                # 3) add novelty to predictions\n                # 4) calculcate probability for current sentence\n                # 5) add sentence_num_state with the weight which is equal to probability to summary_representation\n                # novelty = h^T(j) * Wr * tanh(sj) \n                # sj = sum(h(i)*P(i)) i=(1,j-1)\n                h_sentence = outputs[:, [sentence_num], :]\n                tmp = self.novelty_linear_layer(torch.tanh(summary_representation[:, sentence_num, :])).unsqueeze(2)\n                novelty[:, sentence_num] = torch.bmm(h_sentence, tmp).squeeze(2).squeeze(1)\n                probabilities[:, sentence_num] = torch.sigmoid(predictions[:, sentence_num].clone() + novelty[:, sentence_num].clone())\n                summary_representation[:, sentence_num, :] = (outputs[:, :sentence_num, :].clone() * probabilities[:, :sentence_num].unsqueeze(2).clone()).sum(dim=1)\n\n        result = predictions - novelty\n        result = torch.cat([-result.unsqueeze(1), result.unsqueeze(1)], dim=1) # for cross entropy loss\n        return result","metadata":{"id":"iW7iS76KeEdO","execution":{"iopub.status.busy":"2022-07-23T19:45:08.041809Z","iopub.execute_input":"2022-07-23T19:45:08.042354Z","iopub.status.idle":"2022-07-23T19:45:08.069165Z","shell.execute_reply.started":"2022-07-23T19:45:08.042318Z","shell.execute_reply":"2022-07-23T19:45:08.068199Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Model\n$P\\left(y_{j} = 1 \\mid \\mathbf{h}_{j}, \\mathbf{s}_{j}, \\mathbf{d}\\right)=\\sigma\\left(W_{c} \\mathbf{h}_{j} + \\mathbf{h}_{j}^{T} W_{s} \\mathbf{d}\\right)$\n--------------------","metadata":{"id":"QxpL3AtOrctD"}},{"cell_type":"code","source":"import imp\nimport matplotlib\nimport time\n\nimport math\nimport torch\nimport re\nimport razdel\nmatplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\ndef train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None):\n    model.train()\n    \n    epoch_loss = 0\n    history = []\n    for i, batch in enumerate(iterator):\n        pred = model(batch['inputs'])\n        loss = criterion(pred, batch['outputs'])\n    \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        history.append(loss.item())\n        if (i+1)%10==0:\n            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n\n            clear_output(True)\n            ax[0].plot(history, label='train loss')\n            ax[0].set_xlabel('Batch')\n            ax[0].set_title('Train loss')\n            if train_history is not None:\n                ax[1].plot(train_history, label='general train history')\n                ax[1].set_xlabel('Epoch')\n            if valid_history is not None:\n                ax[1].plot(valid_history, label='general valid history')\n            plt.legend()\n            \n            plt.show()\n\n        \n    return epoch_loss / len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    \n    epoch_loss = 0\n    history = []\n    \n    with torch.no_grad():\n    \n        for i, batch in enumerate(iterator):\n            pred = model(batch['inputs'])\n            loss = criterion(pred, batch['outputs'])\n            \n            epoch_loss += loss.item()\n    return epoch_loss / len(iterator)\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\ndef train_with_logs(model, train_iterator, valid_iterator, optimizer, criterion, N_EPOCHS, CLIP):\n    train_history = []\n    valid_history = []\n    \n    \n    best_valid_loss = float('inf')\n    \n    for epoch in range(N_EPOCHS):\n        \n        start_time = time.time()\n        \n        train_loss = train(model, train_iterator, optimizer, criterion, CLIP, train_history, valid_history)\n        valid_loss = evaluate(model, valid_iterator, criterion)\n        \n        end_time = time.time()\n        \n        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        \n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'best-val-model.pt')\n        \n        train_history.append(train_loss)\n        valid_history.append(valid_loss)\n        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n        \ndef punct_detokenize(text):\n    text = text.strip()\n    punctuation = \",.!?:;%\"\n    closing_punctuation = \")]}\"\n    opening_punctuation = \"([}\"\n    for ch in punctuation + closing_punctuation:\n        text = text.replace(\" \" + ch, ch)\n    for ch in opening_punctuation:\n        text = text.replace(ch + \" \", ch)\n    res = [r'\"\\s[^\"]+\\s\"', r\"'\\s[^']+\\s'\"]\n    for r in res:\n        for f in re.findall(r, text, re.U):\n            text = text.replace(f, f[0] + f[2:-2] + f[-1])\n    text = text.replace(\"' s\", \"'s\").replace(\" 's\", \"'s\")\n    text = text.strip()\n    return text\n\n\ndef postprocess(ref, hyp, is_multiple_ref=False, detokenize_after=False, tokenize_after=True):\n    if is_multiple_ref:\n        reference_sents = ref.split(\" s_s \")\n        decoded_sents = hyp.split(\"s_s\")\n        hyp = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in decoded_sents]\n        ref = [w.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").strip() for w in reference_sents]\n        hyp = \" \".join(hyp)\n        ref = \" \".join(ref)\n    ref = ref.strip()\n    hyp = hyp.strip()\n    if detokenize_after:\n        hyp = punct_detokenize(hyp)\n        ref = punct_detokenize(ref)\n    if tokenize_after:\n        hyp = hyp.replace(\"@@UNKNOWN@@\", \"<unk>\")\n        hyp = \" \".join([token.text for token in razdel.tokenize(hyp)])\n        ref = \" \".join([token.text for token in razdel.tokenize(ref)])\n    return ref, hyp","metadata":{"id":"w2idZnAV1kal","execution":{"iopub.status.busy":"2022-07-23T19:45:08.070681Z","iopub.execute_input":"2022-07-23T19:45:08.071353Z","iopub.status.idle":"2022-07-23T19:45:08.100357Z","shell.execute_reply.started":"2022-07-23T19:45:08.071317Z","shell.execute_reply":"2022-07-23T19:45:08.099423Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocabulary)\nmodel = SentenceTaggerRNN(vocab_size, use_novelty=False).to(device)\n\nparams_count = np.sum([p.numel() for p in model.parameters() if p.requires_grad])\nprint(\"Trainable params: {}\".format(params_count))","metadata":{"id":"-QC1ZmuQfB7f","outputId":"78ef6100-5dbe-45c9-9af9-799b227ae61c","execution":{"iopub.status.busy":"2022-07-23T19:45:31.950719Z","iopub.execute_input":"2022-07-23T19:45:31.951082Z","iopub.status.idle":"2022-07-23T19:45:31.988611Z","shell.execute_reply.started":"2022-07-23T19:45:31.951052Z","shell.execute_reply":"2022-07-23T19:45:31.987537Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 15\nCLIP = 1\n\ndef start_train(use_class_weights, N_EPOCHS, CLIP, lr=1e-5):\n    optimizer = optim.Adam(model.parameters(), lr)\n    if use_class_weights:\n        # weights depend on the number of objects of class 0 and 1\n        weights = {0: 0, 1: 0}\n        for batch in train_iterator:\n          counts = torch.unique(batch['outputs'], return_counts=True)\n          batch_dict = dict(zip(counts[0].cpu().numpy(), counts[1].cpu().numpy()))\n          weights[0] += batch_dict[0]\n          weights[1] += batch_dict[1]\n        total = weights[1] + weights[0]\n        # check order\n        print(\"criterion with weights: \", weights)\n        weights = torch.Tensor(np.array([weights[0] / total, weights[1] / total])).to(device)\n        criterion = nn.CrossEntropyLoss(ignore_index=train_iterator.pad_index, weight=weights)\n    else:\n        criterion = nn.CrossEntropyLoss(ignore_index=train_iterator.pad_index)\n    train_with_logs(model, train_iterator, val_iterator, optimizer, criterion, N_EPOCHS, CLIP)\n\nstart_train(False, N_EPOCHS, CLIP)","metadata":{"id":"rwrhG4v71yts","outputId":"ddb98903-110a-412a-a73d-8aa3db3a786f","execution":{"iopub.status.busy":"2022-07-23T19:45:32.106031Z","iopub.execute_input":"2022-07-23T19:45:32.106970Z","iopub.status.idle":"2022-07-23T19:47:47.829193Z","shell.execute_reply.started":"2022-07-23T19:45:32.106907Z","shell.execute_reply":"2022-07-23T19:47:47.828120Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"vuv4Sh2Vj5Yr"}},{"cell_type":"code","source":"def inference_summarunner(model, iterator, top_k=3):\n\n    references = []\n    predictions = []\n\n    model.eval()\n    for batch in iterator:\n\n        logits = model(batch['inputs'])\n        logits = logits[:, 1, :]  # batch_size, classes(0, 1), num_sentences\n        logits = logits * torch.tensor(batch['outputs'] != iterator.pad_index).float()\n        logits[logits == 0.] = float(\"-Inf\")\n        sum_in = torch.argsort(logits, dim=1)[:, -top_k:]\n        for i in range(len(batch['outputs'])):\n\n            summary = batch['records'][i]['summary'].lower()\n            pred_summary = ' '.join([batch['records'][i]['sentences'][ind] for ind in sum_in.sort(dim=1)[0][i]])\n\n            summary, pred_summary = postprocess(summary, pred_summary)\n\n            references.append(summary)\n            predictions.append(pred_summary)\n    calc_scores(references, predictions)\n\n# model.load_state_dict(torch.load('best-val-model.pt'))\ninference_summarunner(model, test_iterator, 3)","metadata":{"id":"wZ96X37bb_PV","execution":{"iopub.status.busy":"2022-07-23T19:49:09.797549Z","iopub.execute_input":"2022-07-23T19:49:09.797893Z","iopub.status.idle":"2022-07-23T19:49:11.789413Z","shell.execute_reply.started":"2022-07-23T19:49:09.797864Z","shell.execute_reply":"2022-07-23T19:49:11.788372Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Вывод:","metadata":{"id":"HGc4IV8dzyBP"}},{"cell_type":"markdown","source":"В результате экспериментов наилучший вариант без использования novelty составляющей, где BLEU составил 0.4483 против 0.4470 (использование novelty).\nВключение weights в crossentropy loss не влияло на качество, а в большей степени оставляло его таким же\n","metadata":{"id":"3A8kfbwhRcui"}},{"cell_type":"code","source":"","metadata":{"id":"w7mm8_vuiRyX"},"execution_count":null,"outputs":[]}]}